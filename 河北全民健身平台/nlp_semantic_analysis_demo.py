#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NLPè¯­ä¹‰åˆ†ææ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯å°†æ–‡æœ¬æè¿°è½¬æ¢ä¸ºé‡åŒ–æŒ‡æ ‡
"""

import json
import sys
from pathlib import Path
from loguru import logger

# æ·»åŠ æ¨¡å—è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from data_processing.nlp.text_analyzer import PolicyTextAnalyzer, FitnessTextProcessor

# é…ç½®æ—¥å¿—
log_file = "logs/semantic_analysis_demo.log"
logger.add(log_file, rotation="10 MB", level="DEBUG", encoding="utf-8")


def demo_text_to_indicators():
    """æ¼”ç¤ºï¼šæ–‡æœ¬æè¿°è½¬æ¢ä¸ºé‡åŒ–æŒ‡æ ‡"""
    
    logger.info("=" * 80)
    logger.info("æ¼”ç¤º1: æ–‡æœ¬æè¿°è½¬æ¢ä¸ºé‡åŒ–æŒ‡æ ‡")
    logger.info("=" * 80)
    
    analyzer = PolicyTextAnalyzer()
    
    # ç¤ºä¾‹æ”¿ç­–æ–‡æœ¬
    policy_texts = [
        "åˆ°2025å¹´,ç»å¸¸å‚åŠ ä½“è‚²é”»ç‚¼äººæ•°æ¯”ä¾‹è¾¾åˆ°38.5%",
        "äººå‡ä½“è‚²åœºåœ°é¢ç§¯è¾¾åˆ°2.6å¹³æ–¹ç±³",
        "å¿(å¸‚ã€åŒº)ã€ä¹¡é•‡(è¡—é“)ã€è¡Œæ”¿æ‘(ç¤¾åŒº)ä¸‰çº§å…¬å…±å¥èº«è®¾æ–½è¦†ç›–ç‡è¾¾åˆ°100%",
        "æ–°å»ºä½“è‚²åœºé¦†50åº§ï¼ŒæŠ•èµ„é‡‘é¢5.2äº¿å…ƒ"
    ]
    
    for text in policy_texts:
        logger.info(f"\nåŸå§‹æ–‡æœ¬: {text}")
        indicators = analyzer.quantify_policy_indicators(text)
        logger.info(f"é‡åŒ–ç»“æœ: {json.dumps(indicators, ensure_ascii=False, indent=2)}")


def demo_semantic_scoring():
    """æ¼”ç¤ºï¼šè¯­ä¹‰åˆ†æè½¬è¯„åˆ†"""
    
    logger.info("\n" + "=" * 80)
    logger.info("æ¼”ç¤º2: è¯­ä¹‰åˆ†æè½¬è¯„åˆ† (æ–‡æœ¬è¯„ä»· -> é‡åŒ–åˆ†æ•°)")
    logger.info("=" * 80)
    
    analyzer = PolicyTextAnalyzer()
    
    # ä¸åŒè¯„ä»·çš„æ–‡æœ¬
    evaluation_texts = [
        ("ä½“è‚²è®¾æ–½åˆ†å¸ƒéå¸¸å‡è¡¡ï¼Œè¦†ç›–ç‡ä¼˜ç§€ï¼Œç¾¤ä¼—æ»¡æ„åº¦å¾ˆé«˜", "accessibility"),
        ("è®¾æ–½åˆ†å¸ƒè¾ƒå¥½ï¼Œä½†éƒ¨åˆ†åœ°åŒºä»éœ€æ”¹å–„å’ŒåŠ å¼º", "balance"),
        ("è®¾æ–½ä¸¥é‡ä¸è¶³ï¼Œåˆ†å¸ƒå¾ˆä¸å‡è¡¡ï¼Œå­˜åœ¨è¾ƒå¤§å·®è·", "quality"),
        ("åŸºæœ¬æ»¡è¶³éœ€æ±‚ï¼Œæ•´ä½“å°šå¯ï¼Œæœ‰å¾…è¿›ä¸€æ­¥æå‡", "coverage")
    ]
    
    for text, indicator_type in evaluation_texts:
        logger.info(f"\nè¯„ä»·æ–‡æœ¬: {text}")
        logger.info(f"æŒ‡æ ‡ç±»å‹: {indicator_type}")
        score = analyzer.semantic_analysis_to_score(text, indicator_type)
        logger.info(f"é‡åŒ–åˆ†æ•°: {score:.2f}/100")


def demo_facility_data_analysis():
    """æ¼”ç¤ºï¼šåœºé¦†æ•°æ®çš„NLPåˆ†æ"""
    
    logger.info("\n" + "=" * 80)
    logger.info("æ¼”ç¤º3: åœºé¦†æ•°æ®çš„NLPè¯­ä¹‰åˆ†æ")
    logger.info("=" * 80)
    
    # è¯»å–è½¬æ¢åçš„æ•°æ®
    try:
        with open('fitness_facilities_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        facilities = data['facilities']
        logger.info(f"åŠ è½½åœºé¦†æ•°æ®: {len(facilities)} ä¸ªåœºé¦†")
        
        analyzer = PolicyTextAnalyzer()
        
        # åˆ†æå‰5ä¸ªåœºé¦†
        for i, facility in enumerate(facilities[:5], 1):
            logger.info(f"\n--- åœºé¦† {i}: {facility['name']} ---")
            
            # æ„å»ºæè¿°æ–‡æœ¬
            desc_text = f"{facility['description']} {facility.get('remarks', '')}"
            
            # æå–å…³é”®è¯
            if desc_text.strip():
                keywords = analyzer.extract_keywords(desc_text, topK=5)
                logger.info(f"å…³é”®è¯: {[kw[0] for kw in keywords]}")
            
            # æå–æ•°å€¼æŒ‡æ ‡
            indicators = analyzer.extract_numeric_indicators(desc_text)
            logger.info(f"æå–çš„æ•°å€¼æŒ‡æ ‡: {len(indicators)} ä¸ª")
            
            # æ˜¾ç¤ºåœºé¦†é‡åŒ–æŒ‡æ ‡
            logger.info(f"åœºåœ°é¢ç§¯: {facility['indicators']['site_area']} å¹³æ–¹ç±³")
            logger.info(f"æ—¥å®¢æµé‡: {facility['indicators']['daily_visitors']} äººæ¬¡")
            logger.info(f"å»ºæˆå¹´ä»½: {facility['build_year']}")
            
    except FileNotFoundError:
        logger.error("æœªæ‰¾åˆ° fitness_facilities_data.json æ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œ convert_dataset.py")


def demo_comprehensive_analysis():
    """æ¼”ç¤ºï¼šç»¼åˆåˆ†æ - åŸºäºåœºé¦†æ•°æ®è®¡ç®—åŒºåŸŸæŒ‡æ ‡"""
    
    logger.info("\n" + "=" * 80)
    logger.info("æ¼”ç¤º4: ç»¼åˆåˆ†æ - åŒºåŸŸä½“è‚²è®¾æ–½æŒ‡æ ‡é‡åŒ–")
    logger.info("=" * 80)
    
    try:
        with open('fitness_facilities_data.json', 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        facilities = data['facilities']
        
        # æŒ‰åŸå¸‚ç»Ÿè®¡
        city_stats = {}
        
        for facility in facilities:
            city = facility['location'].get('city', 'æœªçŸ¥')
            if not city:
                continue
            
            if city not in city_stats:
                city_stats[city] = {
                    'count': 0,
                    'total_area': 0,
                    'total_visitors': 0,
                    'facilities': []
                }
            
            city_stats[city]['count'] += 1
            city_stats[city]['total_area'] += facility['indicators'].get('site_area', 0)
            city_stats[city]['total_visitors'] += facility['indicators'].get('daily_visitors', 0)
            city_stats[city]['facilities'].append(facility['name'])
        
        # è¾“å‡ºç»Ÿè®¡ç»“æœ
        logger.info("\nå„åŸå¸‚ä½“è‚²è®¾æ–½é‡åŒ–æŒ‡æ ‡:")
        for city, stats in sorted(city_stats.items(), key=lambda x: x[1]['count'], reverse=True)[:10]:
            logger.info(f"\n{city}:")
            logger.info(f"  åœºé¦†æ•°é‡: {stats['count']} ä¸ª")
            logger.info(f"  æ€»åœºåœ°é¢ç§¯: {stats['total_area']:,.2f} å¹³æ–¹ç±³")
            logger.info(f"  æ—¥å‡å®¢æµ: {stats['total_visitors']:,} äººæ¬¡")
            logger.info(f"  äººå‡åœºåœ°é¢ç§¯(å‡è®¾äººå£100ä¸‡): {stats['total_area']/1000000:.2f} å¹³æ–¹ç±³/äºº")
            
            # ä½¿ç”¨NLPè¯­ä¹‰åˆ†æè¯„ä¼°
            analyzer = PolicyTextAnalyzer()
            
            # æ ¹æ®æ•°é‡å’Œé¢ç§¯ç”Ÿæˆè¯„ä»·æ–‡æœ¬
            if stats['count'] >= 20:
                eval_text = "è®¾æ–½æ•°é‡å……åˆ†ï¼Œåˆ†å¸ƒä¼˜ç§€"
            elif stats['count'] >= 10:
                eval_text = "è®¾æ–½æ•°é‡è‰¯å¥½ï¼Œåˆ†å¸ƒè¾ƒå¥½"
            else:
                eval_text = "è®¾æ–½æ•°é‡æœ‰å¾…æå‡ï¼Œéœ€è¦åŠ å¼º"
            
            score = analyzer.semantic_analysis_to_score(eval_text, "facility_coverage")
            logger.info(f"  è®¾æ–½è¦†ç›–è¯„åˆ†: {score:.2f}/100 (åŸºäº: {eval_text})")
        
    except FileNotFoundError:
        logger.error("æœªæ‰¾åˆ° fitness_facilities_data.json æ–‡ä»¶")


def main():
    """ä¸»å‡½æ•°"""
    
    logger.info("ğŸš€ å¼€å§‹NLPè¯­ä¹‰åˆ†ææ¼”ç¤º")
    logger.info("=" * 80)
    
    try:
        # æ¼”ç¤º1: æ–‡æœ¬è½¬æŒ‡æ ‡
        demo_text_to_indicators()
        
        # æ¼”ç¤º2: è¯­ä¹‰è¯„åˆ†
        demo_semantic_scoring()
        
        # æ¼”ç¤º3: åœºé¦†æ•°æ®åˆ†æ
        demo_facility_data_analysis()
        
        # æ¼”ç¤º4: ç»¼åˆåˆ†æ
        demo_comprehensive_analysis()
        
        logger.info("\n" + "=" * 80)
        logger.info("ğŸ‰ NLPè¯­ä¹‰åˆ†ææ¼”ç¤ºå®Œæˆ!")
        logger.info(f"è¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: {log_file}")
        logger.info("=" * 80)
        
    except Exception as e:
        logger.error(f"âŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()
