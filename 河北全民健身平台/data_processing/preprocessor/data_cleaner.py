"""
æ•°æ®æ¸…æ´—ä¸é¢„å¤„ç†æ¨¡å—
"""
from typing import Dict, List, Optional
from loguru import logger
import json
import re


class DataCleaner:
    """æ•°æ®æ¸…æ´—å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self):
        logger.info("åˆå§‹åŒ–æ•°æ®æ¸…æ´—å™¨")
    
    def load_data(self, filepath: str) -> pd.DataFrame:
        """åŠ è½½æ•°æ®"""
        logger.info(f"åŠ è½½æ•°æ®: {filepath}")
        
        if filepath.endswith('.json'):
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            df = pd.DataFrame(data)
        elif filepath.endswith('.csv'):
            df = pd.DataFrame.read_csv(filepath)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {filepath}")
        
        logger.info(f"æ•°æ®å½¢çŠ¶: {df.shape}")
        return df
    
    def remove_duplicates(self, df: pd.DataFrame) -> pd.DataFrame:
        """å»é™¤é‡å¤æ•°æ®"""
        before_count = len(df)
        df = df.drop_duplicates()
        after_count = len(df)
        logger.info(f"å»é‡: {before_count} -> {after_count}, åˆ é™¤ {before_count - after_count} æ¡")
        return df
    
    def handle_missing_values(self, df: pd.DataFrame, strategy: str = 'mean') -> pd.DataFrame:
        """å¤„ç†ç¼ºå¤±å€¼"""
        missing_count = df.isnull().sum().sum()
        logger.info(f"ç¼ºå¤±å€¼æ€»æ•°: {missing_count}")
        
        if missing_count == 0:
            return df
        
        # æ•°å€¼å‹åˆ—ä½¿ç”¨å‡å€¼/ä¸­ä½æ•°å¡«å……
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if strategy == 'mean':
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())
        elif strategy == 'median':
            df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())
        elif strategy == 'zero':
            df[numeric_cols] = df[numeric_cols].fillna(0)
        
        # åˆ†ç±»å‹åˆ—ä½¿ç”¨ä¼—æ•°å¡«å……
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            df[col] = df[col].fillna(df[col].mode()[0] if not df[col].mode().empty else 'Unknown')
        
        logger.info(f"ç¼ºå¤±å€¼å¤„ç†å®Œæˆ,ç­–ç•¥: {strategy}")
        return df
    
    def detect_outliers(self, df: pd.DataFrame, columns: List[str], method: str = 'iqr') -> pd.DataFrame:
        """æ£€æµ‹å¼‚å¸¸å€¼"""
        outliers_count = 0
        
        for col in columns:
            if col not in df.columns:
                continue
            
            if method == 'iqr':
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                outliers_count += len(outliers)
                
                # æ›¿æ¢å¼‚å¸¸å€¼ä¸ºè¾¹ç•Œå€¼
                df.loc[df[col] < lower_bound, col] = lower_bound
                df.loc[df[col] > upper_bound, col] = upper_bound
            
            elif method == 'zscore':
                z_scores = np.abs((df[col] - df[col].mean()) / df[col].std())
                outliers = df[z_scores > 3]
                outliers_count += len(outliers)
                
                # æ›¿æ¢å¼‚å¸¸å€¼ä¸ºå‡å€¼
                df.loc[z_scores > 3, col] = df[col].mean()
        
        logger.info(f"æ£€æµ‹åˆ° {outliers_count} ä¸ªå¼‚å¸¸å€¼,å·²å¤„ç†")
        return df
    
    def normalize_data(self, df: pd.DataFrame, columns: List[str], method: str = 'standard') -> pd.DataFrame:
        """æ•°æ®æ ‡å‡†åŒ–"""
        if method == 'standard':
            df[columns] = self.scaler.fit_transform(df[columns])
            logger.info(f"æ ‡å‡†åŒ–å®Œæˆ(Z-score): {columns}")
        elif method == 'minmax':
            df[columns] = self.minmax_scaler.fit_transform(df[columns])
            logger.info(f"å½’ä¸€åŒ–å®Œæˆ(Min-Max): {columns}")
        
        return df
    
    def encode_categorical(self, df: pd.DataFrame, columns: List[str]) -> pd.DataFrame:
        """åˆ†ç±»å˜é‡ç¼–ç """
        for col in columns:
            if col in df.columns:
                # One-Hotç¼–ç 
                dummies = pd.get_dummies(df[col], prefix=col)
                df = pd.concat([df, dummies], axis=1)
                df = df.drop(col, axis=1)
                logger.info(f"ç¼–ç å®Œæˆ: {col}")
        
        return df
    
    def save_cleaned_data(self, df: pd.DataFrame, filepath: str):
        """ä¿å­˜æ¸…æ´—åçš„æ•°æ®"""
        if filepath.endswith('.csv'):
            df.to_csv(filepath, index=False, encoding='utf-8')
        elif filepath.endswith('.json'):
            df.to_json(filepath, orient='records', force_ascii=False, indent=2)
        
        logger.info(f"æ¸…æ´—åæ•°æ®å·²ä¿å­˜: {filepath}")


class FitnessDataPreprocessor:
    """å¥èº«æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self):
        self.cleaner = DataCleaner()
    
    def preprocess_facility_data(self, input_file: str, output_file: str):
        """é¢„å¤„ç†å¥èº«è®¾æ–½æ•°æ®"""
        logger.info("å¼€å§‹é¢„å¤„ç†å¥èº«è®¾æ–½æ•°æ®...")
        
        # åŠ è½½æ•°æ®
        df = self.cleaner.load_data(input_file)
        
        # å»é‡
        df = self.cleaner.remove_duplicates(df)
        
        # å¤„ç†ç¼ºå¤±å€¼
        df = self.cleaner.handle_missing_values(df, strategy='mean')
        
        # æ£€æµ‹å¼‚å¸¸å€¼
        numeric_cols = ['area', 'capacity', 'investment', 'annual_visitors']
        df = self.cleaner.detect_outliers(df, numeric_cols, method='iqr')
        
        # æ·»åŠ æ´¾ç”Ÿç‰¹å¾
        df['per_capita_area'] = df['area'] / df['capacity']
        df['utilization_rate'] = df['annual_visitors'] / (df['capacity'] * 365)
        
        # ä¿å­˜
        self.cleaner.save_cleaned_data(df, output_file)
        logger.info("âœ… å¥èº«è®¾æ–½æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        return df
    
    def preprocess_population_data(self, input_file: str, output_file: str):
        """é¢„å¤„ç†äººå£æ•°æ®"""
        logger.info("å¼€å§‹é¢„å¤„ç†äººå£æ•°æ®...")
        
        df = self.cleaner.load_data(input_file)
        df = self.cleaner.remove_duplicates(df)
        df = self.cleaner.handle_missing_values(df)
        
        # è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡
        df['urbanization_rate'] = df['urban_population'] / df['total_population']
        df['aging_rate'] = df['age_65_plus'] / df['total_population']
        df['working_age_rate'] = df['age_15_64'] / df['total_population']
        
        self.cleaner.save_cleaned_data(df, output_file)
        logger.info("âœ… äººå£æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        return df
    
    def preprocess_participation_data(self, input_file: str, output_file: str):
        """é¢„å¤„ç†å‚ä¸æ•°æ®"""
        logger.info("å¼€å§‹é¢„å¤„ç†å‚ä¸æ•°æ®...")
        
        df = self.cleaner.load_data(input_file)
        df = self.cleaner.remove_duplicates(df)
        df = self.cleaner.handle_missing_values(df)
        
        # è®¡ç®—å¥èº«å¼ºåº¦æŒ‡æ•°
        df['fitness_intensity'] = df['weekly_frequency'] * df['avg_duration'] / 60
        
        # æ ‡å‡†åŒ–å‚ä¸ç‡
        df['participation_rate_normalized'] = self.cleaner.minmax_scaler.fit_transform(
            df[['participation_rate']]
        )
        
        self.cleaner.save_cleaned_data(df, output_file)
        logger.info("âœ… å‚ä¸æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        return df


if __name__ == "__main__":
    preprocessor = FitnessDataPreprocessor()
    
    # é¢„å¤„ç†å„ç±»æ•°æ®
    preprocessor.preprocess_facility_data(
        "data/raw/facilities.json",
        "data/processed/facilities_cleaned.json"
    )
    
    preprocessor.preprocess_population_data(
        "data/raw/population.json",
        "data/processed/population_cleaned.json"
    )
    
    preprocessor.preprocess_participation_data(
        "data/raw/participation.json",
        "data/processed/participation_cleaned.json"
    )
    
    logger.info("ğŸ‰ æ‰€æœ‰æ•°æ®é¢„å¤„ç†å®Œæˆ!")
